# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 논문 분석 및 해석

## ABSTRACT
모델이 커질수록 full fine-tuning 방식은 현실적으로 어려워진다.
예를 들어 GPT-3 175B 모델을 독립적으로 미세 조정하여 배포하는 것은 **비용적으로 불가능**하다.

### 기존 Fine-Tuning 방식의 문제점
- **매우 많은 파라미터 업데이트 필요**
- **학습 과정에서 GPU 메모리를 많이 사용하며 시간이 너무 오래 걸림**
- **작업별로 새로운 모델이 필요함** → 특정 작업에 맞춰 모델을 fine-tuning할 경우, 작업마다 새로운 가중치를 저장해야 하므로 효율성이 떨어짐

### LoRA의 핵심 아이디어
해당 논문에서 제시하는 **LoRA (Low-Rank Adaptation)** 방법은 **사전 학습된 모델의 가중치를 고정한 채로**, Transformer 아키텍처의 각 층에 **학습 가능한 저순위(rank decomposition) 행렬을 삽입**하는 방식을 사용하여 다운스트림 작업을 위한 학습해야 할 파라미터 수를 대폭 줄인다.

### 논문의 주요 실험 결과
1. **GPT-3 175B 모델을 Adam 옵티마이저로 미세 조정하는 것과 비교했을 때, LoRA는 학습해야 할 파라미터 수를 10,000배 줄이고, GPU 메모리 요구량을 3배 감소시킬 수 있다.**
2. **RoBERTa, DeBERTa, GPT-2, GPT-3 모델에서 전체 미세 조정보다 적은 학습 가능한 파라미터를 사용하면서도 동등하거나 더 나은 성능을 보이며, 더 높은 학습 처리량을 제공한다.**
3. **어댑터(adapters)와 달리 LoRA는 추론(inference) 시 추가적인 지연(latency)이 발생하지 않는다는 장점을 갖는다.**

## INTRODUCTION
### 기존 작업
일부 파라미터만 조정하거나 새로운 task를 위한 외부 모듈을 학습하는 방식으로 모델 배포 문제 해결
- 모델 깊이를 확장하거나 모델의 사용 가능한 시퀀스 길이를 줄여서 추론 지연을 유발합니다
- 종종 미세 조정 기준선에 미치지 못해 효율성과 모델 품질간의 trade-off 초래